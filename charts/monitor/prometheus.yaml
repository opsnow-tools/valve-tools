# chart-repo: stable/prometheus
# chart-version: 10.4.0
# chart-ingress: false
# chart-pvc: prometheus-server ReadWriteOnce 8Gi
# chart-pvc: prometheus-alertmanager ReadWriteOnce 2Gi

nameOverride: prometheus

server:
  ## Prometheus server container image
  image:
    repository: prom/prometheus

  #:ING:service:
  #:ING:  type: SERVICE_TYPE

  #:ING:ingress:
  #:ING:  enabled: INGRESS_ENABLED
  #:ING:  annotations:
  #:ING:    kubernetes.io/ingress.class: nginx
  #:ING:    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  #:ING:  hosts:
  #:ING:    - INGRESS_DOMAIN

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  #podDisruptionBudget:
    #enabled: false
    #maxUnavailable: 1

  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
  #replicaCount: 1

  #statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    #enabled: false

    #annotations: {}
    #labels: {}
    #podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    #headless:
      #annotations: {}
      #labels: {}
      #servicePort: 80

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  #resources:
    #limits:
      #cpu: 500m
      #memory: 512Mi
    #requests:
      #cpu: 500m
      #memory: 512Mi

  ## Prometheus data retention period (default if not specified is 15 days)
  retention: "15d"

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    accessModes:
      - ReadWriteOnce

    size: 8Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, 
    ## choosing the default provisioner. (gp2 on AWS, standard on GKE, AWS & OpenStack)
    #:EFS:storageClass: "efs"

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: prometheus-server

alertmanager:
  ## If false, alertmanager will not be installed
  enabled: true

  ## alertmanager container image
  image:
    repository: prom/alertmanager

  #:ING:service:
  #:ING:  type: SERVICE_TYPE

  #:ING:ingress:
  #:ING:  enabled: INGRESS_ENABLED
  #:ING:  annotations:
  #:ING:    kubernetes.io/ingress.class: nginx
  #:ING:    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  #:ING:  hosts:
  #:ING:    - alertmanager-INGRESS_DOMAIN

  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
  #replicaCount: 1

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    enabled: false

    podManagementPolicy: OrderedReady

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  #resources:
    #limits:
      #cpu: 10m
      #memory: 32Mi
    #requests:
      #cpu: 10m
      #memory: 32Mi

  persistentVolume:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 2Gi
    #:EFS:storageClass: "efs"
    existingClaim: prometheus-alertmanager

kubeStateMetrics:
  ## If false, kube-state-metrics will not be installed
  enabled: KUBE_STATE_METRICS

  ## kube-state-metrics container image
  image:
    repository: quay.io/coreos/kube-state-metrics

  replicaCount: 1

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  #podDisruptionBudget:
    #enabled: false
    #maxUnavailable: 1

  ## kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  #resources:
    #limits:
      #cpu: 10m
      #memory: 16Mi
    #requests:
      #cpu: 10m
      #memory: 16Mi

## Prometheus server ConfigMap entries
serverFiles:

  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerting_rules.yml:
    groups:
      - name: InstanceCountChanged
        rules:
          - alert: InstanceCountChanged
            expr: count (kube_node_labels{node=~"^.*$"}) - count (kube_node_labels{node=~"^.*$"} offset 2m) != 0
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: 'Instance Count Changed'
              description: 'The number of instances changed. (delta: {{ $value }})'
      - name: InstanceDown
        rules:
          - alert: InstanceDown
            expr: up{job="kubernetes-nodes"} == 0
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: 'Instance Down'
              description: 'The instance({{ $labels.instance }}) is down.'
      - name: HighCpuUsage
        rules:
          - alert: HighCpuUsage
            expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{job="kubernetes-service-endpoints",mode="idle"}[5m])) * 100) > 70
            for: 5m
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: 'High CPU Usage(> 70%)'
              description: 'The CPU usage of the instance({{ $labels.instance }}) has exceeded 70 percent for more than 5 minutes.'
      - name: HighMemoryUsage
        rules:
          - alert: HighMemoryUsage
            expr: (node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes) / node_memory_MemTotal_bytes * 100 > 90
            for: 5m
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: 'High Memory Usage(> 90%)'
              description: 'The memory usage of the instance({{ $labels.instance }}) has exceeds 90 percent for more than 5 minutes.'
      - name: PodCrashingLooping
        rules:
          - alert: PodCrashingLooping
            expr: round(increase(kube_pod_container_status_restarts_total[30m])) > 0
            for: 5m
            labels:
              severity: Critical
              cluster: CLUSTER_NAME
            annotations:
              summary: "Pod Crash Looping(> 30m)"
              description: 'Namespace : {{ $labels.namespace }} Pod : {{ $labels.pod }} -- crash {{ $value }} times'
      - name: KubeNodeNotReady
        rules:
          - alert: KubeNodeNotReady
            expr: kube_node_status_condition{job="kubernetes-service-endpoints",condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: Critical
              cluster: CLUSTER_NAME
            annotations:
              summary: "Kube Node Fail :  {{ $labels.condition }}"
              description: "Node {{ $labels.node }} is failed. Check node!!"
      - name: AvgResponseTime
        rules:
          - alert: AvgResponseTime
            expr: (sum(rate(nginx_ingress_controller_response_duration_seconds_sum[5m])) by (host) !=0) / (sum(rate(nginx_ingress_controller_response_duration_seconds_count[5m])) by (host) !=0) > 5
            for: 5m
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: "Average Response Time(> 5s)"
              description: "{{ $labels.host }}'s Average Response Time is over 5sec"
      - name: HPAMaxUsage
        rules:
          - alert: HPAMaxUsage
            expr: (kube_hpa_status_current_replicas) / (kube_hpa_spec_max_replicas) == 1
            for: 5m
            labels:
              severity: Warning
              cluster: CLUSTER_NAME
            annotations:
              summary: "HPA Max Usage"
              description: "{{ $labels.hpa }} is using HPA Max."

  ## Records configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
  #recording_rules.yml: {}

## alertmanager ConfigMap entries
## https://prometheus.io/docs/alerting/configuration/
alertmanagerFiles:
  alertmanager.yml:
    global:
      slack_api_url: 'https://hooks.slack.com/services/SLACK_TOKEN'
    receivers:
      - name: default-receiver
        slack_configs:
          - channel: '#alerts'
            send_resolved: true
            username: '{{ template "slack.default.username" . }}'
            color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
            title: '{{ template "slack.default.title" . }}'
            title_link: '{{ template "slack.default.titlelink" . }}'
            pretext: '{{ .CommonAnnotations.summary }}'
            text: |-
              {{ range .Alerts }}
                *Cluster:* {{ .Labels.cluster }}
                *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
                *Description:* {{ .Annotations.description }}
                *Details:*
                {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                {{ end }}
              {{ end }}
            fallback: '{{ template "slack.default.fallback" . }}'
            icon_emoji: '{{ template "slack.default.iconemoji" . }}'
            icon_url: '{{ template "slack.default.iconurl" }}'
    route:
      ## How long to initially wait to send a notification for a group
      ## of alerts. Allows to wait for an inhibiting alert to arrive or collect
      ## more initial alerts for the same group. (Usually ~0s to few minutes.)
      group_wait: 10s
      ## How long to wait before sending a notification about new alerts that
      ## are added to a group of alerts for which an initial notification has
      ## already been sent. (Usually ~5m or more.)
      group_interval: 1m
      receiver: default-receiver
      ## How long to wait before sending a notification again if it has already
      ## been sent successfully for an alert. (Usually ~3h or more).
      repeat_interval: 8h
